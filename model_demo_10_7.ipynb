{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to predict IC50 from BindingDB data using a DeepPurpose model. There's a lot of room for tweaking things here, but the model performance with minimal code and a relatively short training time is surprisingly good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T00:45:22.716480Z",
     "start_time": "2020-10-01T00:45:19.674465Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from DeepPurpose import utils, dataset, CompoundPred\n",
    "from DeepPurpose import DTI as models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing\n",
    "\n",
    "The entirety of BindingDB is saved as a .tsv file. The function `process_BindingDB` loads it into a Pandas dataframe with three columns: drugs (SMILES format), targets (protein primary structure), and binding score (in this case IC50). The `convert_to_log` flag makes it pIC50.\n",
    "\n",
    "TODO: what are the other arguments to `process_BindingDB`?\n",
    "can I save the dataframe for easier access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T00:47:06.216978Z",
     "start_time": "2020-10-01T00:45:23.274467Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset from path...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 772572: expected 193 fields, saw 205\\nSkipping line 772598: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 805291: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 827961: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 1231688: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1345591: expected 193 fields, saw 241\\nSkipping line 1345592: expected 193 fields, saw 241\\nSkipping line 1345593: expected 193 fields, saw 241\\nSkipping line 1345594: expected 193 fields, saw 241\\nSkipping line 1345595: expected 193 fields, saw 241\\nSkipping line 1345596: expected 193 fields, saw 241\\nSkipping line 1345597: expected 193 fields, saw 241\\nSkipping line 1345598: expected 193 fields, saw 241\\nSkipping line 1345599: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1358864: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 1378087: expected 193 fields, saw 241\\nSkipping line 1378088: expected 193 fields, saw 241\\nSkipping line 1378089: expected 193 fields, saw 241\\nSkipping line 1378090: expected 193 fields, saw 241\\nSkipping line 1378091: expected 193 fields, saw 241\\nSkipping line 1378092: expected 193 fields, saw 241\\nSkipping line 1378093: expected 193 fields, saw 241\\nSkipping line 1378094: expected 193 fields, saw 241\\nSkipping line 1378095: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1417264: expected 193 fields, saw 205\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "There are 1073803 drug target pairs.\n",
      "Default set to logspace (nM -> p) for easier regression\n"
     ]
    }
   ],
   "source": [
    "data_path = './data//BindingDB_All.tsv'\n",
    "X_drugs, X_targets, y = dataset.process_BindingDB(path = data_path, df = None, y = 'IC50', binary = False, convert_to_log = True, threshold = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below just shows what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T00:47:06.277978Z",
     "start_time": "2020-10-01T00:47:06.264967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug 1: Cc1nc(CN2CCN(CC2)c2c(Cl)cnc3[nH]c(nc23)-c2cn(C)nc2C)no1\n",
      "Target 1: MALIPDLAMETWLLLAVSLVLLYLYGTHSHGLFKKLGIPGPTPLPFLGNILSYHKGFCMFDMECHKKYGKVWGFYDGQQPVLAITDPDMIKTVLVKECYSVFTNRRPFGPVGFMKSAISIAEDEEWKRLRSLLSPTFTSGKLKEMVPIIAQYGDVLVRNLRREAETGKPVTLKDVFGAYSMDVITSTSFGVNIDSLNNPQDPFVENTKKLLRFDFLDPFFLSITVFPFLIPILEVLNICVFPREVTNFLRKSVKRMKESRLEDTQKHRVDFLQLMIDSQNSKETESHKALSDLELVAQSIIFIFAGYETTSSVLSFIMYELATHPDVQQKLQEEIDAVLPNKAPPTYDTVLQMEYLDMVVNETLRLFPIAMRLERVCKKDVEINGMFIPKGVVVMIPSYALHRDPKYWTEPEKFLPERFSKKNKDNIDPYIYTPFGSGPRNCIGMRFALMNMKLALIRVLQNFSFKPCKETQIPLKLSLGGLLQPEKPVVLKVESRDGTVSGA\n",
      "IC50 1: 4.301029127075886\n"
     ]
    }
   ],
   "source": [
    "print('Drug 1: ' + X_drugs[0])\n",
    "print('Target 1: ' + X_targets[0])\n",
    "print('IC50 1: ' + str(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug and target encoding\n",
    "\n",
    "DeepPurpose supports several possible encodings for drugs and targets. I suspect choosing and possibly combining these will be key for feature engineering and model performance. Right now I'm using the Morgan Extended-Connectivity Fingerprints encoding for drugs and the conjoint triad features encoding for targets because they're not computationally intensive. I would like to find a way to allow models to take PaDEL features as drug encodings, which is what the gastric cancer paper did. They also encoded targets in terms of mutation position, which may not be feasible for all targets, but is probably worth looking into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T00:56:38.390926Z",
     "start_time": "2020-10-01T00:56:38.374927Z"
    }
   },
   "outputs": [],
   "source": [
    "drug_encoding, target_encoding = 'CNN', 'Transformer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell splits the data into training and testing sets. I think these numbers are probably fine, but maybe there's a way to improve on them. One thing to note is that for some reason RDKit (the biochemistry library) has trouble translating some of the SMILES data (~500 drugs) into Morgan format, so I might want to look into fixing that. That's only a small fraction of the drugs in the training data, though.\n",
    "\n",
    "TODO: look into `split_method` options,\n",
    "i would also like to save the training and testing data sets since this step takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T00:59:41.995150Z",
     "start_time": "2020-10-01T00:56:40.459926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 1073803 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 549205\n",
      "encoding protein...\n",
      "unique target sequence: 5078\n",
      "splitting dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train, val, test = utils.data_process(X_drugs, X_targets, y, \n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='cold_drug',frac=[0.7,0.1,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "\n",
    "DeepPurpose's model configuration utility is a wrapper for generating neural networks using PyTorch. The list of options for hyperparameters is [here](https://github.com/kexinhuang12345/DeepPurpose/blob/e169e2f550694145077bb2af95a4031abe400a77/DeepPurpose/utils.py#L486). Several types of model architecture are supported including CNNs, RNNs, MPNNs, MLPs, and transformers. I think there is a lot of potential work to be done on hyperparameter optimization here. The hyperparameters used below are suggested defaults that aren't too computationally intensive; they produce a 3 layer MPNN. I would also like to incorporate some models that don't utilize deep learning, as I think with proper input feature engineering we may be able to get good performance with an SVM or ridge regressor, and it would be nice to have a more interpretable model for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:40:21.499496Z",
     "start_time": "2020-09-29T17:40:21.483498Z"
    }
   },
   "outputs": [],
   "source": [
    "config = utils.generate_config(drug_encoding = drug_encoding, \n",
    "                               target_encoding = target_encoding,\n",
    "                         cls_hidden_dims = [1024,1024,512], \n",
    "                         train_epoch = 3, \n",
    "                         LR = 0.001, \n",
    "                         batch_size = 128,\n",
    "                         hidden_dim_drug = 128,\n",
    "                         mpnn_hidden_size = 128,\n",
    "                         mpnn_depth = 3\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:40:22.221677Z",
     "start_time": "2020-09-29T17:40:22.188516Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.model_initialize(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and loading\n",
    "\n",
    "Using the hyperparameters above, the model takes about 20 minutes on a GPU (1.5 hours on a laptop CPU) to train on BindingDB. For demo purposes I'm going to just load a model I trained earlier today, but I used the exact same code as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:02:14.115643Z",
     "start_time": "2020-09-29T17:40:24.629496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU!\n",
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 43.6444. Total time 0.0 hours\n",
      "Training at Epoch 1 iteration 100 with loss 1.86518. Total time 0.00194 hours\n",
      "Training at Epoch 1 iteration 200 with loss 1.71702. Total time 0.00361 hours\n",
      "Training at Epoch 1 iteration 300 with loss 1.46526. Total time 0.00527 hours\n",
      "Training at Epoch 1 iteration 400 with loss 1.54277. Total time 0.00666 hours\n",
      "Training at Epoch 1 iteration 500 with loss 1.43693. Total time 0.00833 hours\n",
      "Training at Epoch 1 iteration 600 with loss 1.43159. Total time 0.01 hours\n",
      "Training at Epoch 1 iteration 700 with loss 1.73679. Total time 0.01138 hours\n",
      "Training at Epoch 1 iteration 800 with loss 1.35257. Total time 0.01305 hours\n",
      "Training at Epoch 1 iteration 900 with loss 1.32767. Total time 0.01472 hours\n",
      "Training at Epoch 1 iteration 1000 with loss 1.45166. Total time 0.01611 hours\n",
      "Training at Epoch 1 iteration 1100 with loss 1.38412. Total time 0.01777 hours\n",
      "Training at Epoch 1 iteration 1200 with loss 1.23958. Total time 0.01916 hours\n",
      "Training at Epoch 1 iteration 1300 with loss 1.57378. Total time 0.02083 hours\n",
      "Training at Epoch 1 iteration 1400 with loss 1.35599. Total time 0.0225 hours\n",
      "Training at Epoch 1 iteration 1500 with loss 1.62974. Total time 0.02388 hours\n",
      "Training at Epoch 1 iteration 1600 with loss 1.38078. Total time 0.02555 hours\n",
      "Training at Epoch 1 iteration 1700 with loss 0.99957. Total time 0.02694 hours\n",
      "Training at Epoch 1 iteration 1800 with loss 1.44862. Total time 0.02861 hours\n",
      "Training at Epoch 1 iteration 1900 with loss 0.90433. Total time 0.03 hours\n",
      "Training at Epoch 1 iteration 2000 with loss 1.26077. Total time 0.03166 hours\n",
      "Training at Epoch 1 iteration 2100 with loss 0.88527. Total time 0.03333 hours\n",
      "Training at Epoch 1 iteration 2200 with loss 0.95161. Total time 0.03472 hours\n",
      "Training at Epoch 1 iteration 2300 with loss 1.04718. Total time 0.03638 hours\n",
      "Training at Epoch 1 iteration 2400 with loss 0.94137. Total time 0.03805 hours\n",
      "Training at Epoch 1 iteration 2500 with loss 1.51039. Total time 0.03944 hours\n",
      "Training at Epoch 1 iteration 2600 with loss 1.20930. Total time 0.04111 hours\n",
      "Training at Epoch 1 iteration 2700 with loss 1.35165. Total time 0.0425 hours\n",
      "Training at Epoch 1 iteration 2800 with loss 1.08155. Total time 0.04416 hours\n",
      "Training at Epoch 1 iteration 2900 with loss 1.07081. Total time 0.04555 hours\n",
      "Training at Epoch 1 iteration 3000 with loss 0.89513. Total time 0.04722 hours\n",
      "Training at Epoch 1 iteration 3100 with loss 1.22521. Total time 0.04861 hours\n",
      "Training at Epoch 1 iteration 3200 with loss 0.98097. Total time 0.05027 hours\n",
      "Training at Epoch 1 iteration 3300 with loss 1.39838. Total time 0.05166 hours\n",
      "Training at Epoch 1 iteration 3400 with loss 1.36200. Total time 0.05333 hours\n",
      "Training at Epoch 1 iteration 3500 with loss 0.97811. Total time 0.055 hours\n",
      "Training at Epoch 1 iteration 3600 with loss 1.01150. Total time 0.05638 hours\n",
      "Training at Epoch 1 iteration 3700 with loss 1.15817. Total time 0.05805 hours\n",
      "Training at Epoch 1 iteration 3800 with loss 1.26655. Total time 0.05944 hours\n",
      "Training at Epoch 1 iteration 3900 with loss 0.98351. Total time 0.06111 hours\n",
      "Training at Epoch 1 iteration 4000 with loss 1.05346. Total time 0.06277 hours\n",
      "Training at Epoch 1 iteration 4100 with loss 1.19558. Total time 0.06416 hours\n",
      "Training at Epoch 1 iteration 4200 with loss 1.02394. Total time 0.06583 hours\n",
      "Training at Epoch 1 iteration 4300 with loss 0.76668. Total time 0.06722 hours\n",
      "Training at Epoch 1 iteration 4400 with loss 1.06996. Total time 0.06888 hours\n",
      "Training at Epoch 1 iteration 4500 with loss 1.06926. Total time 0.07055 hours\n",
      "Training at Epoch 1 iteration 4600 with loss 1.14702. Total time 0.07194 hours\n",
      "Training at Epoch 1 iteration 4700 with loss 0.99601. Total time 0.07361 hours\n",
      "Training at Epoch 1 iteration 4800 with loss 0.98203. Total time 0.075 hours\n",
      "Training at Epoch 1 iteration 4900 with loss 1.25130. Total time 0.07666 hours\n",
      "Training at Epoch 1 iteration 5000 with loss 0.80980. Total time 0.07805 hours\n",
      "Training at Epoch 1 iteration 5100 with loss 0.92562. Total time 0.08 hours\n",
      "Training at Epoch 1 iteration 5200 with loss 0.98315. Total time 0.08166 hours\n",
      "Training at Epoch 1 iteration 5300 with loss 0.89206. Total time 0.08333 hours\n",
      "Training at Epoch 1 iteration 5400 with loss 0.99093. Total time 0.08527 hours\n",
      "Training at Epoch 1 iteration 5500 with loss 1.00264. Total time 0.08722 hours\n",
      "Training at Epoch 1 iteration 5600 with loss 0.84922. Total time 0.08916 hours\n",
      "Training at Epoch 1 iteration 5700 with loss 0.97237. Total time 0.09111 hours\n",
      "Training at Epoch 1 iteration 5800 with loss 1.19060. Total time 0.09305 hours\n",
      "Validation at Epoch 1 , MSE: 0.99704 , Pearson Correlation: 0.75748 with p-value: 0.0 , Concordance Index: 0.78175\n",
      "Training at Epoch 2 iteration 0 with loss 0.88358. Total time 0.11 hours\n",
      "Training at Epoch 2 iteration 100 with loss 1.05656. Total time 0.11166 hours\n",
      "Training at Epoch 2 iteration 200 with loss 0.70060. Total time 0.11361 hours\n",
      "Training at Epoch 2 iteration 300 with loss 0.86501. Total time 0.11527 hours\n",
      "Training at Epoch 2 iteration 400 with loss 0.80967. Total time 0.11722 hours\n",
      "Training at Epoch 2 iteration 500 with loss 0.70455. Total time 0.11944 hours\n",
      "Training at Epoch 2 iteration 600 with loss 1.00680. Total time 0.12305 hours\n",
      "Training at Epoch 2 iteration 700 with loss 0.79539. Total time 0.12472 hours\n",
      "Training at Epoch 2 iteration 800 with loss 1.31490. Total time 0.12638 hours\n",
      "Training at Epoch 2 iteration 900 with loss 0.97850. Total time 0.12805 hours\n",
      "Training at Epoch 2 iteration 1000 with loss 0.78986. Total time 0.13 hours\n",
      "Training at Epoch 2 iteration 1100 with loss 0.85736. Total time 0.13166 hours\n",
      "Training at Epoch 2 iteration 1200 with loss 0.69204. Total time 0.13333 hours\n",
      "Training at Epoch 2 iteration 1300 with loss 0.85076. Total time 0.13472 hours\n",
      "Training at Epoch 2 iteration 1400 with loss 0.99110. Total time 0.13638 hours\n",
      "Training at Epoch 2 iteration 1500 with loss 0.97593. Total time 0.13805 hours\n",
      "Training at Epoch 2 iteration 1600 with loss 0.80899. Total time 0.13944 hours\n",
      "Training at Epoch 2 iteration 1700 with loss 1.02329. Total time 0.14111 hours\n",
      "Training at Epoch 2 iteration 1800 with loss 0.82464. Total time 0.14277 hours\n",
      "Training at Epoch 2 iteration 1900 with loss 0.86134. Total time 0.14444 hours\n",
      "Training at Epoch 2 iteration 2000 with loss 0.85820. Total time 0.14611 hours\n",
      "Training at Epoch 2 iteration 2100 with loss 0.70147. Total time 0.14777 hours\n",
      "Training at Epoch 2 iteration 2200 with loss 0.73523. Total time 0.14944 hours\n",
      "Training at Epoch 2 iteration 2300 with loss 0.88861. Total time 0.15083 hours\n",
      "Training at Epoch 2 iteration 2400 with loss 1.38566. Total time 0.1525 hours\n",
      "Training at Epoch 2 iteration 2500 with loss 0.96615. Total time 0.15416 hours\n",
      "Training at Epoch 2 iteration 2600 with loss 1.11014. Total time 0.15583 hours\n",
      "Training at Epoch 2 iteration 2700 with loss 0.89777. Total time 0.1575 hours\n",
      "Training at Epoch 2 iteration 2800 with loss 0.95179. Total time 0.15944 hours\n",
      "Training at Epoch 2 iteration 2900 with loss 0.83598. Total time 0.16111 hours\n",
      "Training at Epoch 2 iteration 3000 with loss 0.95264. Total time 0.16277 hours\n",
      "Training at Epoch 2 iteration 3100 with loss 0.96271. Total time 0.16444 hours\n",
      "Training at Epoch 2 iteration 3200 with loss 0.95231. Total time 0.16611 hours\n",
      "Training at Epoch 2 iteration 3300 with loss 0.78767. Total time 0.16777 hours\n",
      "Training at Epoch 2 iteration 3400 with loss 0.90262. Total time 0.16944 hours\n",
      "Training at Epoch 2 iteration 3500 with loss 0.80737. Total time 0.17083 hours\n",
      "Training at Epoch 2 iteration 3600 with loss 1.00314. Total time 0.1725 hours\n",
      "Training at Epoch 2 iteration 3700 with loss 0.65361. Total time 0.17416 hours\n",
      "Training at Epoch 2 iteration 3800 with loss 0.79839. Total time 0.17583 hours\n",
      "Training at Epoch 2 iteration 3900 with loss 0.54866. Total time 0.1775 hours\n",
      "Training at Epoch 2 iteration 4000 with loss 0.79991. Total time 0.17916 hours\n",
      "Training at Epoch 2 iteration 4100 with loss 0.90398. Total time 0.18083 hours\n",
      "Training at Epoch 2 iteration 4200 with loss 0.79354. Total time 0.18222 hours\n",
      "Training at Epoch 2 iteration 4300 with loss 0.73273. Total time 0.18416 hours\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 2 iteration 4400 with loss 0.81172. Total time 0.18583 hours\n",
      "Training at Epoch 2 iteration 4500 with loss 0.79346. Total time 0.1875 hours\n",
      "Training at Epoch 2 iteration 4600 with loss 0.78172. Total time 0.18916 hours\n",
      "Training at Epoch 2 iteration 4700 with loss 0.80029. Total time 0.19083 hours\n",
      "Training at Epoch 2 iteration 4800 with loss 0.94989. Total time 0.1925 hours\n",
      "Training at Epoch 2 iteration 4900 with loss 1.01614. Total time 0.19416 hours\n",
      "Training at Epoch 2 iteration 5000 with loss 0.71070. Total time 0.19583 hours\n",
      "Training at Epoch 2 iteration 5100 with loss 0.66127. Total time 0.1975 hours\n",
      "Training at Epoch 2 iteration 5200 with loss 0.94067. Total time 0.19916 hours\n",
      "Training at Epoch 2 iteration 5300 with loss 0.70135. Total time 0.20083 hours\n",
      "Training at Epoch 2 iteration 5400 with loss 0.75143. Total time 0.2025 hours\n",
      "Training at Epoch 2 iteration 5500 with loss 0.97586. Total time 0.20416 hours\n",
      "Training at Epoch 2 iteration 5600 with loss 0.68279. Total time 0.20583 hours\n",
      "Training at Epoch 2 iteration 5700 with loss 0.81410. Total time 0.2075 hours\n",
      "Training at Epoch 2 iteration 5800 with loss 1.02342. Total time 0.20916 hours\n",
      "Validation at Epoch 2 , MSE: 0.87177 , Pearson Correlation: 0.78700 with p-value: 0.0 , Concordance Index: 0.79816\n",
      "Training at Epoch 3 iteration 0 with loss 0.62747. Total time 0.22333 hours\n",
      "Training at Epoch 3 iteration 100 with loss 0.55064. Total time 0.225 hours\n",
      "Training at Epoch 3 iteration 200 with loss 0.87085. Total time 0.22638 hours\n",
      "Training at Epoch 3 iteration 300 with loss 0.88506. Total time 0.22805 hours\n",
      "Training at Epoch 3 iteration 400 with loss 0.84277. Total time 0.22972 hours\n",
      "Training at Epoch 3 iteration 500 with loss 0.59267. Total time 0.23111 hours\n",
      "Training at Epoch 3 iteration 600 with loss 0.93255. Total time 0.23277 hours\n",
      "Training at Epoch 3 iteration 700 with loss 0.82583. Total time 0.23444 hours\n",
      "Training at Epoch 3 iteration 800 with loss 0.83151. Total time 0.23583 hours\n",
      "Training at Epoch 3 iteration 900 with loss 0.63438. Total time 0.2375 hours\n",
      "Training at Epoch 3 iteration 1000 with loss 0.68053. Total time 0.23888 hours\n",
      "Training at Epoch 3 iteration 1100 with loss 0.91162. Total time 0.24055 hours\n",
      "Training at Epoch 3 iteration 1200 with loss 0.53711. Total time 0.24222 hours\n",
      "Training at Epoch 3 iteration 1300 with loss 0.49140. Total time 0.24361 hours\n",
      "Training at Epoch 3 iteration 1400 with loss 0.61276. Total time 0.24527 hours\n",
      "Training at Epoch 3 iteration 1500 with loss 0.78669. Total time 0.24694 hours\n",
      "Training at Epoch 3 iteration 1600 with loss 0.56840. Total time 0.24833 hours\n",
      "Training at Epoch 3 iteration 1700 with loss 0.66134. Total time 0.25 hours\n",
      "Training at Epoch 3 iteration 1800 with loss 0.74423. Total time 0.25166 hours\n",
      "Training at Epoch 3 iteration 1900 with loss 0.74823. Total time 0.25333 hours\n",
      "Training at Epoch 3 iteration 2000 with loss 0.55154. Total time 0.255 hours\n",
      "Training at Epoch 3 iteration 2100 with loss 0.76963. Total time 0.25666 hours\n",
      "Training at Epoch 3 iteration 2200 with loss 0.60674. Total time 0.25833 hours\n",
      "Training at Epoch 3 iteration 2300 with loss 0.64179. Total time 0.26 hours\n",
      "Training at Epoch 3 iteration 2400 with loss 0.82568. Total time 0.26138 hours\n",
      "Training at Epoch 3 iteration 2500 with loss 0.79423. Total time 0.26305 hours\n",
      "Training at Epoch 3 iteration 2600 with loss 0.73258. Total time 0.26472 hours\n",
      "Training at Epoch 3 iteration 2700 with loss 0.66790. Total time 0.26611 hours\n",
      "Training at Epoch 3 iteration 2800 with loss 0.77796. Total time 0.26777 hours\n",
      "Training at Epoch 3 iteration 2900 with loss 0.72836. Total time 0.26944 hours\n",
      "Training at Epoch 3 iteration 3000 with loss 0.63947. Total time 0.27083 hours\n",
      "Training at Epoch 3 iteration 3100 with loss 0.76226. Total time 0.2725 hours\n",
      "Training at Epoch 3 iteration 3200 with loss 0.63438. Total time 0.27416 hours\n",
      "Training at Epoch 3 iteration 3300 with loss 0.71340. Total time 0.27555 hours\n",
      "Training at Epoch 3 iteration 3400 with loss 0.61109. Total time 0.27722 hours\n",
      "Training at Epoch 3 iteration 3500 with loss 0.75732. Total time 0.27888 hours\n",
      "Training at Epoch 3 iteration 3600 with loss 0.69368. Total time 0.28027 hours\n",
      "Training at Epoch 3 iteration 3700 with loss 0.53281. Total time 0.28194 hours\n",
      "Training at Epoch 3 iteration 3800 with loss 0.64660. Total time 0.28361 hours\n",
      "Training at Epoch 3 iteration 3900 with loss 0.61089. Total time 0.285 hours\n",
      "Training at Epoch 3 iteration 4000 with loss 0.75433. Total time 0.28666 hours\n",
      "Training at Epoch 3 iteration 4100 with loss 0.61056. Total time 0.28805 hours\n",
      "Training at Epoch 3 iteration 4200 with loss 0.70868. Total time 0.28972 hours\n",
      "Training at Epoch 3 iteration 4300 with loss 0.92623. Total time 0.29138 hours\n",
      "Training at Epoch 3 iteration 4400 with loss 0.60869. Total time 0.29277 hours\n",
      "Training at Epoch 3 iteration 4500 with loss 0.85526. Total time 0.29444 hours\n",
      "Training at Epoch 3 iteration 4600 with loss 0.75383. Total time 0.29611 hours\n",
      "Training at Epoch 3 iteration 4700 with loss 0.69336. Total time 0.2975 hours\n",
      "Training at Epoch 3 iteration 4800 with loss 0.68831. Total time 0.29916 hours\n",
      "Training at Epoch 3 iteration 4900 with loss 0.55379. Total time 0.30111 hours\n",
      "Training at Epoch 3 iteration 5000 with loss 0.61451. Total time 0.30277 hours\n",
      "Training at Epoch 3 iteration 5100 with loss 0.68981. Total time 0.30444 hours\n",
      "Training at Epoch 3 iteration 5200 with loss 0.59770. Total time 0.30611 hours\n",
      "Training at Epoch 3 iteration 5300 with loss 0.60691. Total time 0.30777 hours\n",
      "Training at Epoch 3 iteration 5400 with loss 0.68549. Total time 0.30944 hours\n",
      "Training at Epoch 3 iteration 5500 with loss 0.75011. Total time 0.31083 hours\n",
      "Training at Epoch 3 iteration 5600 with loss 0.85421. Total time 0.31277 hours\n",
      "Training at Epoch 3 iteration 5700 with loss 0.70007. Total time 0.31416 hours\n",
      "Training at Epoch 3 iteration 5800 with loss 0.80770. Total time 0.31583 hours\n",
      "Validation at Epoch 3 , MSE: 0.81978 , Pearson Correlation: 0.80210 with p-value: 0.0 , Concordance Index: 0.80664\n",
      "--- Go for Testing ---\n",
      "Testing MSE: 0.815280586379076 , Pearson Correlation: 0.8045105130834797 with p-value: 0.0 , Concordance Index: 0.8076748978520706\n",
      "--- Training Finished ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run this if you want to train a new one\n",
    "model.train(train, val, test, verbose = True)\n",
    "model.save_model('./model-9-29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T17:38:26.870268Z",
     "start_time": "2020-09-29T17:38:26.643277Z"
    }
   },
   "outputs": [],
   "source": [
    "#run this if you want to use my trained one\n",
    "model = models.model_pretrained(path_dir = './model-9-24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "The following code is built in to the DeepPurpose `train` method, I've just pulled it out so I can grab the dataset that was set aside for validation during the data processing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:03:26.429775Z",
     "start_time": "2020-09-29T18:03:26.408776Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "params = {'batch_size': config['batch_size'],\n",
    "    'shuffle': True,\n",
    "    'num_workers': config['num_workers'],\n",
    "    'drop_last': False}\n",
    "\n",
    "validation_generator = data.DataLoader(utils.data_process_loader(val.index.values, val.Label.values, val, **config), **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available performance metrics are ROC-AUC, PR-AUC, F1, cross-entropy loss, MSE, Pearson Correlation with p-value, and Concordance Index. I'm displaying the ROC-AUC here for comparison to the DeepIC50 paper. It's not at their level yet, but it's also not bad for a first try?\n",
    "\n",
    "TODO: graph ROC curve, also why does test_ return \"logits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:04:17.515059Z",
     "start_time": "2020-09-29T18:03:30.238061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8197822689413422\n"
     ]
    }
   ],
   "source": [
    "model.binary = False\n",
    "auc, auprc, f1, loss, logits = models.DBTA.test_(model, validation_generator, model.model, test=True)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to get the ROC curve to actually plot...\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import sklearn\n",
    "#from sklearn.metrics import mean_squared_error, roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "#y_label = []\n",
    "#roc_auc_file = os.path.join(model.result_folder, \"roc-auc.jpg\")\n",
    "#plt.figure(0)\n",
    "#sklearn.metrics.roc_curve(y_pred, y_label, roc_auc_file, model.drug_encoding)\n",
    "#plt.figure(1)\n",
    "#pr_auc_file = os.path.join(model.result_folder, \"pr-auc.jpg\")\n",
    "#sklearn.metrics.prauc_curve(y_pred, y_label, pr_auc_file, model.drug_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model usage\n",
    "\n",
    "To predict IC50, run data_process to create a dataset consisting of a single drug-target pair, and then run `predict`, which is just a wrapper on `test_`. The output is in pIC50. \n",
    "\n",
    "TODO: why do I need to include `y` as input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 1 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 1\n",
      "encoding protein...\n",
      "unique target sequence: 1\n",
      "splitting dataset...\n",
      "do not do train/test split on the data for already splitted data\n",
      "predicting...\n",
      "The predicted score is [7.395412921905518]\n"
     ]
    }
   ],
   "source": [
    "X_drug = ['CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N']\n",
    "X_target = ['MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQVTVDEVLAEGGFAIVFLVRTSNGMKCALKRMFVNNEHDLQVCKREIQIMRDLSGHKNIVGYIDSSINNVSSGDVWEVLILMDFCRGGQVVNLMNQRLQTGFTENEVLQIFCDTCEAVARLHQCKTPIIHRDLKVENILLHDRGHYVLCDFGSATNKFQNPQTEGVNAVEDEIKKYTTLSYRAPEMVNLYSGKIITTKADIWALGCLLYKLCYFTLPFGESQVAICDGNFTIPDNSRYSQDMHCLIRYMLEPDPDKRPDIYQVSYFSFKLLKKECPIPNVQNSPIPAKLPEPVKASEAAAKKTQPKARLTDPIPTTETSIAPRQRPKAGQTQPNPGILPIQPALTPRKRATVQPPPQAAGSSNQPGLLASVPQPKPQAPPSQPLPQTQAKQPQAPPTPQQTPSTQAQGLPAQAQATPQHQQQLFLKQQQQQQQPPPAQQQPAGTFYQQQQAQTQQFQAVHPATQKPAIAQFPVVSQGGSQQQLMQNFYQQQQQQQQQQQQQQLATALHQQQLMTQQAALQQKPTMAAGQQPQPQPAAAPQPAPAQEPAIQAPVRQQPKVQTTPPPAVQGQKVGSLTPPSSPKTQRAGHRRILSDVTHSAVFGVPASKSTQLLQAAAAEASLNKSKSATTTPSGSPRTSQQNVYNPSEGSTWNPFDDDNFSKLTAEELLNKDFAKLGEGKHPEKLGGSAESLIPGFQSTQGDAFATTSFSAGTAEKRKGGQTVDSGLPLLSVSDPFIPLQVPDAPEKLIEGLKSPDTSLLLPDLLPMTDPFGSTSDAVIEKADVAVESLIPGLEPPVPQRLPSQTESVTSNRTDSLTGEDSLLDCSLLSNPTTDLLEEFAPTAISAPVHKAAEDSNLISGFDVPEGSDKVAEDEFDPIPVLITKNPQGGHSRNSSGSSESSLPNLARSLLLVDQLIDL']\n",
    "X_pred = utils.data_process(X_drug, X_target, y, \n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='no_split')\n",
    "y_pred = model.predict(X_pred)\n",
    "print('The predicted score is ' + str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Random Forest\n",
    "\n",
    "## Data importing\n",
    "\n",
    "TODO: import support for other datasets (incl conversion to SMILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from DeepPurpose.pybiomed_helper import _GetPseudoAAC, CalculateAADipeptideComposition, \\\n",
    "calcPubChemFingerAll, CalculateConjointTriad, GetQuasiSequenceOrder\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "from DeepPurpose.utils import *\n",
    "from DeepPurpose.model_helper import Encoder_MultipleLayers, Embeddings        \n",
    "from DeepPurpose.encoders import *\n",
    "from DeepPurpose import DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data//BindingDB_All.tsv'\n",
    "df = pd.read_csv(data_path, sep = '\\t', error_bad_lines=False)\n",
    "df = df[df['Number of Protein Chains in Target (>1 implies a multichain complex)'] == 1.0]\n",
    "df = df[df['Ligand SMILES'].notnull()]\n",
    "\n",
    "df = df[['Ligand SMILES','Target Source Organism According to Curator or DataSource',\\\n",
    "            'BindingDB Target Chain  Sequence', 'Kd (nM)', 'IC50 (nM)', 'Ki (nM)',\\\n",
    "            'EC50 (nM)','pH','Temp (C)']]\n",
    "df.rename(columns={'Ligand SMILES':'SMILES',\n",
    "                        'BindingDB Target Chain  Sequence': 'Target Sequence',\n",
    "                        'Target Source Organism According to Curator or DataSource': 'Organism',\n",
    "                        'Kd (nM)':'Kd',\n",
    "                        'IC50 (nM)':'IC50',\n",
    "                        'Ki (nM)':'Ki',\n",
    "                        'EC50 (nM)':'EC50',\n",
    "                        'kon (M-1-s-1)':'kon',\n",
    "                        'koff (s-1)':'koff',\n",
    "                        'Temp (C)':'Temp'}, \n",
    "                        inplace=True)\n",
    "df['Temp'] = df['Temp'].str.rstrip('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to include entries that don't have temp and pH:\n",
    "\n",
    "TODO: better imputer, save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer()\n",
    "y_i = imputer.fit_transform(y)\n",
    "y_i = pd.DataFrame(data=y,columns=idx_str+['pH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want entries that include temp and pH:\n",
    "\n",
    "TODO: improve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_str = ['IC50','Temp']\n",
    "df_want = df\n",
    "convert_to_log = 0\n",
    "\n",
    "df_want = df_want[df_want.PubChem_ID.notnull() | df_want.UniProt_ID.notnull()]\n",
    "df_want = df_want[df_want.InChI.notnull()]\n",
    "df_want = df_want[df_want['Temp'].notnull()]\n",
    "df_want = df_want[df_want['pH'].notnull()]\n",
    "df_want = df_want[df_want['IC50'].notnull()]\n",
    "\n",
    "for label in idx_str:\n",
    "    df_want[label] = df_want[label].str.replace('>', '')\n",
    "    df_want[label] = df_want[label].str.replace('<', '')\n",
    "\n",
    "y = df_want[idx_str]\n",
    "y['pH'] = df_want['pH']\n",
    "X_drugs = df_want.SMILES.values\n",
    "X_targets = df_want['Target Sequence'].values\n",
    "y = y.apply(pd.to_numeric, errors='coerce')\n",
    "X_org = df_want['Organism']\n",
    "\n",
    "df_data = y\n",
    "df_data['SMILES'] = X_drugs\n",
    "df_data['Target Sequence'] = X_targets\n",
    "df_data['Organism'] = X_org\n",
    "\n",
    "print('in total: ' + str(len(df_data)) + ' drug-target pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine which output variable you want (IC50, pIC50, magnitude of IC50)\n",
    "\n",
    "TODO: categorical classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in idx_str:\n",
    "    if convert_to_log:\n",
    "            print('Default set to logspace (nM -> p) for easier regression')\n",
    "            y[label] = convert_y_unit(df_want[label].values, 'nM', 'p') \n",
    "    else:\n",
    "            y[label] = df_want[label].values\n",
    "\n",
    "import math\n",
    "def magnitude(x):\n",
    "    if x > 0:\n",
    "        return int(math.floor(math.log10(x)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "magic50 = df_data['IC50'].apply(magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodings for use with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_func_list= [smiles2morgan,drug2emb_encoder,calcPubChemFingerAll,smiles2daylight]\n",
    "column_name = 'SMILES'\n",
    "\n",
    "for func in drug_func_list:\n",
    "    save_column_name = func.__name__\n",
    "    unique = pd.Series(df_data[column_name].unique()).apply(func)\n",
    "    unique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "    df_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_func_list = [CalculateConjointTriad, protein2emb_encoder,target2quasi]\n",
    "column_name = 'Target Sequence'\n",
    "\n",
    "for func in prot_func_list:\n",
    "    save_column_name = func.__name__\n",
    "    AA = pd.Series(df_data[column_name].unique()).apply(func)\n",
    "    AA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "    df_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = pd.get_dummies(df_data['Organism'], prefix='var')\n",
    "df_data1=df_data.join(cat_list)\n",
    "discard=['SMILES','Target Sequence','Organism']\n",
    "df_vars=df_data1.columns.values.tolist()\n",
    "to_keep=[i for i in df_vars if i not in discard]\n",
    "df_final=df_data1[to_keep]\n",
    "df_final.head()\n",
    "X = df_final.drop([\"IC50\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: incorporate results from part 1 here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattener(x):\n",
    "    if isinstance(x, collections.Iterable):\n",
    "        return [a for i in x for a in flattener(i)]\n",
    "    else:\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typelist = []\n",
    "for i in list(X):\n",
    "    print(i, ':', type(X[i].iloc[1]))#, ':', X[i].iloc[1])\n",
    "    if isinstance(X[i].iloc[1],np.ndarray):\n",
    "        print(len(X[i].iloc[1]))\n",
    "        typelist.extend([i]*len(X[i].iloc[1]))\n",
    "    elif isinstance(X[i].iloc[1],tuple):\n",
    "        for n in X[i].iloc[1]:\n",
    "            if isinstance(n,np.ndarray):\n",
    "                print(len(n))\n",
    "                typelist.extend([i]*len(n))\n",
    "    elif i[:4] == 'var_':\n",
    "        typelist.append('organism')\n",
    "    else:\n",
    "        typelist.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_df = np.empty(shape=[len(X),len(flattener(X.iloc[0]))])\n",
    "for n in range(len(X)):\n",
    "    Z_df[n] = flattener(X.iloc[n])\n",
    "    \n",
    "foo2 = np.vstack(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = X.apply(flattener,axis=1)\n",
    "foo = vec.to_numpy(dtype = object)\n",
    "foo2 = np.vstack(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_final[\"IC50\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(foo2, y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=10)\n",
    "rf_model.fit(train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pearsonr(y_train, rf_model.predict(train_scaled))\n",
    "b = pearsonr(y_test, rf_model.predict(test_scaled))\n",
    "print(\"Random Forest train r = \",a)\n",
    "print(\"Random Forest test r = \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'importance':np.round(rf_model2.feature_importances_,3)})\n",
    "out = importances.sort_values('importance',ascending=False)\n",
    "importances[\"Type\"] = typelist\n",
    "type_importance = importances.groupby(by=['Type']).sum()\n",
    "type_importance.sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: save moel, usage (incl for imported data), confidence intervals, recommended drugs per target, data visualization, boruta or whatever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
