{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, train, val, test = None, verbose = True):\n",
    "\t\tif len(train.Label.unique()) == 2:\n",
    "\t\t\tself.binary = True\n",
    "\t\t\tself.config['binary'] = True\n",
    "\n",
    "\t\tlr = self.config['LR']\n",
    "\t\tdecay = self.config['decay']\n",
    "\t\tBATCH_SIZE = self.config['batch_size']\n",
    "\t\ttrain_epoch = self.config['train_epoch']\n",
    "\t\tif 'test_every_X_epoch' in self.config.keys():\n",
    "\t\t\ttest_every_X_epoch = self.config['test_every_X_epoch']\n",
    "\t\telse:     \n",
    "\t\t\ttest_every_X_epoch = 40\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tself.model = self.model.to(self.device)\n",
    "\n",
    "\t\t# support multiple GPUs\n",
    "\t\tif torch.cuda.device_count() > 1:\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(\"Let's use \" + str(torch.cuda.device_count()) + \" GPUs!\")\n",
    "\t\t\tself.model = nn.DataParallel(self.model, dim = 0)\n",
    "\t\telif torch.cuda.device_count() == 1:\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(\"Let's use \" + str(torch.cuda.device_count()) + \" GPU!\")\n",
    "\t\telse:\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(\"Let's use CPU/s!\")\n",
    "\t\t# Future TODO: support multiple optimizers with parameters\n",
    "\t\topt = torch.optim.Adam(self.model.parameters(), lr = lr, weight_decay = decay)\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('--- Data Preparation ---')\n",
    "\n",
    "\t\tparams = {'batch_size': BATCH_SIZE,\n",
    "\t    \t\t'shuffle': True,\n",
    "\t    \t\t'num_workers': self.config['num_workers'],\n",
    "\t    \t\t'drop_last': False}\n",
    "\t\tif (self.drug_encoding == \"MPNN\"):\n",
    "\t\t\tparams['collate_fn'] = mpnn_collate_func\n",
    "\n",
    "\t\ttraining_generator = data.DataLoader(data_process_loader_Property_Prediction(train.index.values, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t train.Label.values, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t train, **self.config), \n",
    "\t\t\t\t\t\t\t\t\t\t\t**params)\n",
    "\t\tvalidation_generator = data.DataLoader(data_process_loader_Property_Prediction(val.index.values, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tval.Label.values, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tval, **self.config), \n",
    "\t\t\t\t\t\t\t\t\t\t\t**params)\n",
    "\t\t\n",
    "\t\tif test is not None:\n",
    "\t\t\tinfo = data_process_loader_Property_Prediction(test.index.values, test.Label.values, test, **self.config) #diff\n",
    "\t\t\tparams_test = {'batch_size': BATCH_SIZE,\n",
    "\t\t\t\t\t'shuffle': False,\n",
    "\t\t\t\t\t'num_workers': self.config['num_workers'],\n",
    "\t\t\t\t\t'drop_last': False,\n",
    "\t\t\t\t\t'sampler':SequentialSampler(info)}\n",
    "        \n",
    "\t\t\tif (self.drug_encoding == \"MPNN\"):\n",
    "\t\t\t\tparams_test['collate_fn'] = mpnn_collate_func\n",
    "                #diff below\n",
    "\t\t\ttesting_generator = data.DataLoader(data_process_loader_Property_Prediction(test.index.values, test.Label.values, test, **self.config), **params_test)\n",
    "\n",
    "\t\t# early stopping\n",
    "\t\tif self.binary:\n",
    "\t\t\tmax_auc = 0\n",
    "\t\telse:\n",
    "\t\t\tmax_MSE = 10000\n",
    "\t\tmodel_max = copy.deepcopy(self.model)\n",
    "\n",
    "\t\tvalid_metric_record = []\n",
    "\t\tvalid_metric_header = [\"# epoch\"] \n",
    "\t\tif self.binary:\n",
    "\t\t\tvalid_metric_header.extend([\"AUROC\", \"AUPRC\", \"F1\"])\n",
    "\t\telse:\n",
    "\t\t\tvalid_metric_header.extend([\"MSE\", \"Pearson Correlation\", \"with p-value\", \"Concordance Index\"])\n",
    "\t\ttable = PrettyTable(valid_metric_header)\n",
    "\t\tfloat2str = lambda x:'%0.4f'%x\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('--- Go for Training ---')\n",
    "\t\tt_start = time() \n",
    "\t\tfor epo in range(train_epoch):\n",
    "\t\t\tfor i, (v_d, label) in enumerate(training_generator):  #diff: v_p missing\n",
    "\t\t\t\tif self.drug_encoding == \"MPNN\" or self.drug_encoding == 'Transformer':\n",
    "\t\t\t\t\tv_d = v_d\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tv_d = v_d.float().to(self.device)                \n",
    "\t\t\t\t\t#score = self.model(v_d, v_p.float().to(self.device))\n",
    "               \n",
    "\t\t\t\tscore = self.model(v_d)\n",
    "\t\t\t\tlabel = Variable(torch.from_numpy(np.array(label)).float()).to(self.device)\n",
    "\n",
    "\t\t\t\tif self.binary:\n",
    "\t\t\t\t\tloss_fct = torch.nn.BCELoss()\n",
    "\t\t\t\t\tm = torch.nn.Sigmoid()\n",
    "\t\t\t\t\tn = torch.squeeze(m(score), 1)\n",
    "\t\t\t\t\tloss = loss_fct(n, label)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tloss_fct = torch.nn.MSELoss()\n",
    "\t\t\t\t\tn = torch.squeeze(score, 1)\n",
    "\t\t\t\t\tloss = loss_fct(n, label)\n",
    "\t\t\t\tloss_history.append(loss.item())\n",
    "\n",
    "\t\t\t\topt.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\topt.step()\n",
    "\n",
    "\t\t\t\tif verbose:\n",
    "\t\t\t\t\tif (i % 100 == 0):\n",
    "\t\t\t\t\t\tt_now = time()\n",
    "\t\t\t\t\t\tif verbose:\n",
    "\t\t\t\t\t\t\tprint('Training at Epoch ' + str(epo + 1) + ' iteration ' + str(i) + \\\n",
    "\t\t\t\t\t\t\t' with loss ' + str(loss.cpu().detach().numpy())[:7] +\\\n",
    "\t\t\t\t\t\t\t\". Total time \" + str(int(t_now - t_start)/3600)[:7] + \" hours\") \n",
    "\t\t\t\t\t\t### record total run time\n",
    "\n",
    "\t\t\t##### validate, select the best model up to now \n",
    "\t\t\twith torch.set_grad_enabled(False):\n",
    "\t\t\t\tif self.binary:  \n",
    "\t\t\t\t\t## binary: ROC-AUC, PR-AUC, F1  \n",
    "\t\t\t\t\tauc, auprc, f1, logits = self.test_(validation_generator, self.model)\n",
    "\t\t\t\t\tlst = [\"epoch \" + str(epo)] + list(map(float2str,[auc, auprc, f1]))\n",
    "\t\t\t\t\tvalid_metric_record.append(lst)\n",
    "\t\t\t\t\tif auc > max_auc:\n",
    "\t\t\t\t\t\tmodel_max = copy.deepcopy(self.model)\n",
    "\t\t\t\t\t\tmax_auc = auc\n",
    "\t\t\t\t\tif verbose:\n",
    "\t\t\t\t\t\tprint('Validation at Epoch '+ str(epo + 1) + ' , AUROC: ' + str(auc)[:7] + \\\n",
    "\t\t\t\t\t\t  ' , AUPRC: ' + str(auprc)[:7] + ' , F1: '+str(f1)[:7]) #diff: no cross-entropy loss\n",
    "\t\t\t\telse:  \n",
    "\t\t\t\t\t### regression: MSE, Pearson Correlation, with p-value, Concordance Index  \n",
    "\t\t\t\t\tmse, r2, p_val, CI, logits = self.test_(validation_generator, self.model)\n",
    "\t\t\t\t\tlst = [\"epoch \" + str(epo)] + list(map(float2str,[mse, r2, p_val, CI]))\n",
    "\t\t\t\t\tvalid_metric_record.append(lst)\n",
    "\t\t\t\t\tif mse < max_MSE:\n",
    "\t\t\t\t\t\tmodel_max = copy.deepcopy(self.model)\n",
    "\t\t\t\t\t\tmax_MSE = mse\n",
    "\t\t\t\t\tif verbose:\n",
    "\t\t\t\t\t\tprint('Validation at Epoch '+ str(epo + 1) + ' , MSE: ' + str(mse)[:7] + ' , Pearson Correlation: '\\\n",
    "\t\t\t\t\t\t + str(r2)[:7] + ' with p-value: ' + str(p_val)[:7] +' , Concordance Index: '+str(CI)[:7])\n",
    "\t\t\ttable.add_row(lst)\n",
    "\n",
    "\n",
    "\t\t#### after training \n",
    "\t\tprettytable_file = os.path.join(self.result_folder, \"valid_markdowntable.txt\")\n",
    "\t\twith open(prettytable_file, 'w') as fp:\n",
    "\t\t\tfp.write(table.get_string())\n",
    "\n",
    "\t\t# load early stopped model\n",
    "\t\tself.model = model_max #diff: this line is before the \"after training\" section\n",
    "\n",
    "\t\tif test is not None:\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint('--- Go for Testing ---')\n",
    "\t\t\tif self.binary:\n",
    "\t\t\t\tauc, auprc, f1, logits = self.test_(testing_generator, model_max, test = True, verbose = verbose)\n",
    "\t\t\t\ttest_table = PrettyTable([\"AUROC\", \"AUPRC\", \"F1\"])\n",
    "\t\t\t\ttest_table.add_row(list(map(float2str, [auc, auprc, f1])))\n",
    "\t\t\t\tif verbose:\n",
    "\t\t\t\t\tprint('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1))\t\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tmse, r2, p_val, CI, logits = self.test_(testing_generator, model_max, test = True, verbose = verbose)\n",
    "\t\t\t\ttest_table = PrettyTable([\"MSE\", \"Pearson Correlation\", \"with p-value\", \"Concordance Index\"])\n",
    "\t\t\t\ttest_table.add_row(list(map(float2str, [mse, r2, p_val, CI])))\n",
    "\t\t\t\tif verbose:\n",
    "\t\t\t\t\tprint('Testing MSE: ' + str(mse) + ' , Pearson Correlation: ' + str(r2) \n",
    "\t\t\t\t\t  + ' with p-value: ' + str(p_val) +' , Concordance Index: '+str(CI))\n",
    "\t\t\tnp.save(os.path.join(self.result_folder, str(self.drug_encoding)\n",
    "\t\t\t\t     + '_logits.npy'), np.array(logits))                \n",
    "\n",
    "\t\t\t######### learning record ###########\n",
    "\n",
    "\t\t\t### 1. test results\n",
    "\t\t\tprettytable_file = os.path.join(self.result_folder, \"test_markdowntable.txt\")\n",
    "\t\t\twith open(prettytable_file, 'w') as fp:\n",
    "\t\t\t\tfp.write(test_table.get_string())\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t### 2. learning curve \n",
    "\t\t\tfontsize = 16\n",
    "\t\t\titer_num = list(range(1,len(loss_history)+1))\n",
    "\t\t\tplt.figure(3)\n",
    "\t\t\tplt.plot(iter_num, loss_history, \"bo-\")\n",
    "\t\t\tplt.xlabel(\"iteration\", fontsize = fontsize)\n",
    "\t\t\tplt.ylabel(\"loss value\", fontsize = fontsize)\n",
    "\t\t\tpkl_file = os.path.join(self.result_folder, \"loss_curve_iter.pkl\")\n",
    "\t\t\twith open(pkl_file, 'wb') as pck:\n",
    "\t\t\t\tpickle.dump(loss_history, pck)\n",
    "\n",
    "\t\t\tfig_file = os.path.join(self.result_folder, \"loss_curve.png\")\n",
    "\t\t\tplt.savefig(fig_file)\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('--- Training Finished ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_drug(df_data, drug_encoding, column_name = 'SMILES', save_column_name = 'drug_encoding'):\n",
    "\tprint('encoding drug...')\n",
    "\tprint('unique drugs: ' + str(len(df_data[column_name].unique())))\n",
    "\n",
    "\tif drug_encoding == 'Morgan':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(smiles2morgan)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\telif drug_encoding == 'Pubchem':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(calcPubChemFingerAll)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\telif drug_encoding == 'Daylight':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(smiles2daylight)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\telif drug_encoding == 'rdkit_2d_normalized':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(smiles2rdkit2d)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\telif drug_encoding == 'CNN':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(trans_drug)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\t\t# the embedding is large and not scalable but quick, so we move to encode in dataloader batch. \n",
    "\telif drug_encoding == 'CNN_RNN':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(trans_drug)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\telif drug_encoding == 'Transformer':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(drug2emb_encoder)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\telif drug_encoding == 'MPNN':\n",
    "\t\tunique = pd.Series(df_data[column_name].unique()).apply(smiles2mpnnfeature)\n",
    "\t\tunique_dict = dict(zip(df_data[column_name].unique(), unique))\n",
    "\t\tdf_data[save_column_name] = [unique_dict[i] for i in df_data[column_name]]\n",
    "\telse:\n",
    "\t\traise AttributeError(\"Please use the correct drug encoding available!\")\n",
    "\treturn df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepPurpose.pybiomed_helper import _GetPseudoAAC, CalculateAADipeptideComposition, \\\n",
    "calcPubChemFingerAll, CalculateConjointTriad, GetQuasiSequenceOrder\n",
    "\n",
    "calcPubChemFingerAll\n",
    "\n",
    "_GetPseudoAAC\n",
    "CalculateAADipeptideComposition\n",
    "CalculateConjointTriad\n",
    "GetQuasiSequenceOrder\n",
    "\n",
    "smiles2morgan\n",
    "smiles2daylight\n",
    "smiles2rdkit2d\n",
    "trans_drug\n",
    "drug2emb_encoder\n",
    "smiles2mpnnfeature\n",
    "\n",
    "trans_protein\n",
    "protein2emb_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_protein(df_data, target_encoding, column_name = 'Target Sequence', save_column_name = 'target_encoding'):\n",
    "\tprint('encoding protein...')\n",
    "\tprint('unique target sequence: ' + str(len(df_data[column_name].unique())))\n",
    "\tif target_encoding == 'AAC':\n",
    "\t\tprint('-- Encoding AAC takes time. Time Reference: 24s for ~100 sequences in a CPU.\\\n",
    "\t\t\t\t Calculate your time by the unique target sequence #, instead of the entire dataset.')\n",
    "\t\tAA = pd.Series(df_data[column_name].unique()).apply(CalculateAADipeptideComposition)\n",
    "\t\tAA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "\t\tdf_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]\n",
    "\telif target_encoding == 'PseudoAAC':\n",
    "\t\tprint('-- Encoding PseudoAAC takes time. Time Reference: 462s for ~100 sequences in a CPU.\\\n",
    "\t\t\t\t Calculate your time by the unique target sequence #, instead of the entire dataset.')\n",
    "\t\tAA = pd.Series(df_data[column_name].unique()).apply(_GetPseudoAAC)\n",
    "\t\tAA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "\t\tdf_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]\n",
    "\telif target_encoding == 'Conjoint_triad':\n",
    "\t\tAA = pd.Series(df_data[column_name].unique()).apply(CalculateConjointTriad)\n",
    "\t\tAA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "\t\tdf_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]\n",
    "\telif target_encoding == 'Quasi-seq':\n",
    "\t\tAA = pd.Series(df_data[column_name].unique()).apply(GetQuasiSequenceOrder)\n",
    "\t\tAA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "\t\tdf_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]\n",
    "\telif target_encoding == 'CNN':\n",
    "\t\tAA = pd.Series(df_data[column_name].unique()).apply(trans_protein)\n",
    "\t\tAA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "\t\tdf_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]\n",
    "\t\t# the embedding is large and not scalable but quick, so we move to encode in dataloader batch. \n",
    "\telif target_encoding == 'CNN_RNN':\n",
    "\t\tAA = pd.Series(df_data[column_name].unique()).apply(trans_protein)\n",
    "\t\tAA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "\t\tdf_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]\n",
    "\telif target_encoding == 'Transformer':\n",
    "\t\tAA = pd.Series(df_data[column_name].unique()).apply(protein2emb_encoder)\n",
    "\t\tAA_dict = dict(zip(df_data[column_name].unique(), AA))\n",
    "\t\tdf_data[save_column_name] = [AA_dict[i] for i in df_data[column_name]]\n",
    "\telse:\n",
    "\t\traise AttributeError(\"Please use the correct protein encoding available!\")\n",
    "\treturn df_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
